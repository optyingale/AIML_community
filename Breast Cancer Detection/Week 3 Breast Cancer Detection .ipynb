{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\srtpa\\Downloads\\cancer dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_Y=LabelEncoder()\n",
    "df.iloc[:,1]=labelencoder_Y.fit_transform(df.iloc[:,1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>842302</td>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>842517</td>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>84300903</td>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>84348301</td>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>84358402</td>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302          1        17.99         10.38          122.80     1001.0   \n",
       "1    842517          1        20.57         17.77          132.90     1326.0   \n",
       "2  84300903          1        19.69         21.25          130.00     1203.0   \n",
       "3  84348301          1        11.42         20.38           77.58      386.1   \n",
       "4  84358402          1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split the original data into dependent and indepndent dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,2:31].values #features that help us determine if patient has cancer or not\n",
    "Y=df.iloc[:,1].values #this is the dataset containing our target variable which indicates diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we use is usually split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model’s prediction on this subset.\n",
    "\n",
    "From Sklearn, sub-library model_selection, we will import the train_test_split so we can split to training and test sets.  The test_size inside the function indicates the percentage of the data that should be held over for testing. It’s usually around 80/20 or 70/30. The ratio is kept as such so that model does not overfit or underfit. \n",
    "Let us understand first what overfitting and underfitting means:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "Overfitting means that model we trained has trained “too well” and is now, well, fit too closely to the training dataset. This usually happens when the model is too complex i.e. too many features/variables compared to the number of observations. This model will be very accurate on the training data but will probably be very not accurate on untrained or new data. It is because this model is not generalized, meaning you can generalize the results and can’t make any inferences on other data, which is, ultimately, what you are trying to do. Basically, when this happens, the model learns or describes the “noise” in the training data instead of the actual relationships between variables in the data. This noise, obviously, isn’t part in of any new dataset, and cannot be applied to it.\n",
    "\n",
    "#### Underfitting\n",
    "In contrast to overfitting, when a model is underfitted, it means that the model does not fit the training data and therefore misses the trends in the data. It also means the model cannot be generalized to new data. This is usually the result of a very simple model which does not have enough predictors/independent variables. It could also happen when, for example, we fit a linear model ,like linear regression to data that is not linear. It almost goes without saying that this model will have poor predictive ability on training data and can’t be generalized to other data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code that you can use to test and split data using scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Now, there is a small homeowrk for statistics wherein you have to read about the parameteres, define them in brief and write about two main types of distributions: \n",
    "#### 1. Gaussian distribution \n",
    "#### 2. Binomial distribution \n",
    "#### Differentiate between both as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "### Numerical Features/Variables can be segmented into Discrete and Continuous \n",
    "##### Discrete can take countable values whereas Continuous can take infinite values\n",
    "\n",
    "### There are 4 main ways for measuring centers\n",
    "##### Mean : Arithmatic average of the dataset (usually denoted by symbol \"myu\" or \"x-bar\"). \n",
    "##### Median : It is the middle data value after arranging the data in ascending order (for even dataset, it is the mean of the two middle values)\n",
    "##### Mode : It is the data that occurs the most frequently in the given dataset\n",
    "##### Mid-Range : It is the average value of the minimum and maximum of the given dataset\n",
    "\n",
    "### Other essential\n",
    "##### Variance : It is the average of the squared differences from the mean (denoted by \"sigma\") (It basically is the summation of the difference of each point in the dataset with the mean of the same dataset, after squaring this summation divide it by number of instances(N)).\n",
    "##### Percentile : It is usually portrayed by 4 values (0th, 25th, 50th, 75th  and 100th percentile) 50th percentile is always the median\n",
    "##### Poisson Distribution : describes the distribution of binary data from an infinite sample. It gives the probability of getting r events in a population whereas Binomial Distribution of binary data is from a finite sample\n",
    "##### Exponential Distribution : widely used for survival analysis. From the expected life of a machine to the expected life of a human, exponential distribution successfully delivers the result. \n",
    "\n",
    "\n",
    "## Difference between Gaussian and Binomial Distribution\n",
    "##### 1. Gaussian (Normal) Distribution is a type of continuous probability distribution for a real-valued random variable. On the other hand Binomial Distribution is for discrete probability distribution, in other words it is not possible to find a value between two data values\n",
    "##### 2. Normal Distribution has a bell shaped graph whereas Binomial Distribution has a stairstep graph (which can look like a bell shaped graph only when the sample size is very big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had written about this on the group as well and shall write it once again that statistics is a very important element of machine learning. While we are working things on Python in this project, you should be working on things in statistics and trying to find statistical conclusions and means in the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To center the data (make it have zero mean and unit standard error), you subtract the mean and then divide the result by the standard deviation.\n",
    "\n",
    "x′=(x−μ)/σ.\n",
    "\n",
    "You do that on the training set of data. But then you have to apply the same transformation to your testing set (e.g. in cross-validation), or to newly obtained examples before forecast. But you have to use the same two parameters μ and σ (values) that you used for centering the training set.\n",
    "\n",
    "Hence, every sklearn's transform's fit() just calculates the parameters (e.g. μ and σ in case of StandardScaler) and saves them as an internal objects state. Afterwards, you can call its transform() method to apply the transformation to a particular set of examples.\n",
    "\n",
    "fit_transform() joins these two steps and is used for the initial fitting of parameters on the training set x, but it also returns a transformed x′. Internally, it just calls first fit() and then transform() on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have applied fit transform on our test and train data. Now it is time to read about various models that can be used to predict whether a cancer cell is beningn or malignant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection is an important part of solving a machine learning but not as important as data cleaning! Your model will always be as good as your data is so focus should always be on getting high quality data and cleaning it properly. \n",
    "\n",
    "There are a number of Machine Learning models avaialble which can be employed to read to meaningful conclusions and selecting the right model depends on a variety of factors such as:\n",
    "\n",
    "1. The accuracy of the model.\n",
    "2. The interpretability of the model.\n",
    "3. The complexity of the model.\n",
    "4. The scalability of the model.\n",
    "5. How long does it take to build, train, and test the model?\n",
    "6. How long does it take to make predictions using the model?\n",
    "7. Does the model meet the business goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will be mainly focussing on three algorithms which can be used to model our dataset:\n",
    "    1. Logistics regression\n",
    "    2. Decision tree classifier \n",
    "    3. Random Forest classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Selectively write 5 lines about each of the above three algorithms so that even a rather inexperienced person can understand it alongwith dealing all the technicalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "### Logistic Regression\n",
    "##### 1. Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable\n",
    "##### 2. It is used for problems where independant variables are continuous in nature and dependant variables are in categorical form (Yes/No, +/-)\n",
    "##### 3. It uses Sigmoid function ( \"sigma\"(t) = 1/(1+e^-t) ) which predicts the category \n",
    "##### 4. There are 3 types of Logistic Regression\n",
    "###### a> Binary Logistic Regression : As explained above\n",
    "###### b> Multinomial Logistic Regression : Three or more categories without considering order\n",
    "###### c> Ordinal Logistic Regression : Three or more categories with ordering\n",
    "##### 5. To predict a partcular class, a threshold can be set also known as Decision Boundary, it can be linear, non-linear (Polynomial order can be increased for complex models)\n",
    "\n",
    "\n",
    "### Decision Tree Classifier\n",
    "##### 1. It is derived from the basic understanding of trees, to understand it more - Essentially it is divided into 3 parts. a> Node, b> Branch, c> Leaf Node\n",
    "##### 2. This tree like structure is built through a process known as binary recursive partitioning. This is a recursive process of splittling data into partitions and furthermore into branches \n",
    "##### 3. In this method a set of training examples is broken down into smaller and smaller subsets while at the same time an associated decision tree gets incrementally developed. At the end of the learning process, a decision tree covering the training set is returned.\n",
    "##### 4. Working\n",
    "##### a> Using the decision algorithm, we start at the tree root and split the data on the feature that results in the largest information gain (IG) \n",
    "##### b> In an iterative process, we can then repeat this splitting procedure at each child node until the leaves are pure. This means that the samples at each leaf node all belong to the same class\n",
    "##### c> To prevent overfitting we can limit the creation of tree to a certain depth that allows the tree to stop at impure roots\n",
    "##### 5. Exclusion of unimportant features, Easy interpretation for small dataset and fast data classification of unseen data are some of the advantages of this algorithm\n",
    "\n",
    "\n",
    "### Random Forest Classifier\n",
    "##### 1. Random forests are an ensemble (collection) learning method for classification where multiple decision trees are generated\n",
    "##### 2. It applies weight concept to the features considering the impact of result from decision tree, tree with low error are given high weightage and vice-versa.\n",
    "##### 3. A basic parameter of this classifier is the number of trees to be generated that governs the time complexity of this algorithm\n",
    "##### 4. Other parameters can be tuned according to necessity for the decision trees generated by random forest model.\n",
    "##### 5. This method tends to give higher accuracy at the cost of computation time and memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's assignment is more theory oriented because communication skills are equally important in Data Science as is coding and Statistics (Mathematics). These questions will help you gain a command over explaining things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all for week 3, we will apply these algorithms in the week 4 and calculate accuracy of the models and the final submission and evaluation will be done after that week. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
