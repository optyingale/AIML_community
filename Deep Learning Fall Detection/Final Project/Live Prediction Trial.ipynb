{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessities\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import posenet\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_RFC = \"RFC.pickle\"\n",
    "filename_ANN = \"ANN\"\n",
    "filename_CNN = \"CNN\"\n",
    "filename_RNN = \"RNN\"\n",
    "filename_LSTM = \"LSTM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading One Hot Encoding categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"OneHotEncoding categories.txt\", 'r') as f:\n",
    "    ohe_categories = f.readline()\n",
    "ohe_categories = list(ohe_categories.split(\"'\")[1:-1])\n",
    "while \" \" in ohe_categories:\n",
    "    ohe_categories.remove(\" \")\n",
    "print(ohe_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Configuring my cudnn for tensorflow 1** <br><br>\n",
    " **Needed for my laptop, unsure for others**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global sess\n",
    "global graph\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "set_session(sess)\n",
    "\n",
    "#model_RFC = pickle.load(open(filename_RFC, 'rb'))\n",
    "model_ANN = tf.keras.models.load_model(filename_ANN)\n",
    "model_CNN = tf.keras.models.load_model(filename_CNN)\n",
    "model_RNN = tf.keras.models.load_model(filename_RNN)\n",
    "model_LSTM = tf.keras.models.load_model(filename_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session = tf.keras.backend.get_session()\n",
    "#init = tf.global_variables_initializer()\n",
    "#session.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "#config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "temporary = pd.DataFrame()\n",
    "seconds=2\n",
    "\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "with graph.as_default():\n",
    "    \n",
    "    set_session(sess)\n",
    "        \n",
    "    # Setting ouput stride to 16\n",
    "    output_stride = 16\n",
    "    model_id = 101\n",
    "    model_cfg, model_outputs = posenet.load_model(model_id, sess)\n",
    "\n",
    "    #cap = cv2.VideoCapture(\"E:\\\\AIML community\\\\Deep Learning Fall Detection\\\\Final Project\\\\data\\\\Online_fall.mp4\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    input_image, display_image, output_scale = posenet.read_cap(cap)\n",
    "    \n",
    "    #file_save = \"live\"\n",
    "    #result = cv2.VideoWriter(f'{file_save}.avi',  \n",
    "    #                     cv2.VideoWriter_fourcc(*'MJPG'), \n",
    "    #                     10, (int(cap.get(3)), int(cap.get(4)))) \n",
    "\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        input_image, display_image, output_scale = posenet.read_cap(cap)\n",
    "\n",
    "        heatmaps_result, offsets_result, displacement_fwd_result, displacement_bwd_result = sess.run(\n",
    "            model_outputs,\n",
    "            feed_dict={'image:0': input_image})\n",
    "\n",
    "        pose_scores, keypoint_scores, keypoint_coords = posenet.decode_multi.decode_multiple_poses(\n",
    "            heatmaps_result.squeeze(axis=0),\n",
    "            offsets_result.squeeze(axis=0),\n",
    "            displacement_fwd_result.squeeze(axis=0),\n",
    "            displacement_bwd_result.squeeze(axis=0),\n",
    "            output_stride=output_stride,\n",
    "            max_pose_detections=10,\n",
    "            min_pose_score=0.15)\n",
    "\n",
    "        keypoint_coords *= output_scale\n",
    "\n",
    "        # TODO this isn't particularly fast, use GL for drawing and display someday...\n",
    "        overlay_image = posenet.draw_skel_and_kp(\n",
    "            display_image, pose_scores, keypoint_scores, keypoint_coords,\n",
    "            min_pose_score=0.15, min_part_score=0.3)\n",
    "        # setting min_pose_score and min_part_score =0 prints the estimated value\n",
    "\n",
    "        x = keypoint_coords[0][:,0]\n",
    "        y = keypoint_coords[0][:,1]\n",
    "\n",
    "        data = []\n",
    "        for i in range(len(posenet.PART_NAMES)):\n",
    "            for j in [x, y]:\n",
    "                data.append(j[i])\n",
    "        #print(temp)\n",
    "\n",
    "        temp = pd.DataFrame(data).T\n",
    "\n",
    "        prediction_RFC = model_RFC.predict(temp)\n",
    "        prediction_ANN = ohe_categories[np.argmax(model_ANN.predict(temp))]\n",
    "        prediction_CNN = \"Not Ready yet\"\n",
    "        prediction_RNN = \"Not Ready yet\"\n",
    "        prediction_LSTM = \"Not Ready yet\"\n",
    "        \n",
    "        temporary = pd.concat([temporary, temp], ignore_index=True)\n",
    "        X_CNN = []\n",
    "        if len(temporary) == seconds*30:\n",
    "            X_CNN.append(temporary.values)\n",
    "            X_CNN = np.asarray(X_CNN).reshape(-1, seconds*30, 34, 1)\n",
    "            prediction_CNN = ohe_categories[np.argmax(model_CNN.predict(X_CNN))]\n",
    "            X_RNN = np.asarray(X_CNN).reshape(-1, seconds*30, 34)\n",
    "            prediction_RNN = ohe_categories[np.argmax(model_RNN.predict(X_RNN))]\n",
    "            prediction_LSTM = ohe_categories[np.argmax(model_LSTM.predict(X_RNN))]\n",
    "            temporary.drop(temporary.index[:1], inplace=True)\n",
    "            \n",
    "        \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "        fontscale = 1\n",
    "        color = (255, 0, 255)   # BGR\n",
    "        thickness = 1\n",
    "        \n",
    "        #coords_RFC = (0, 20)\n",
    "        coords_ANN = (0, 20)\n",
    "        coords_CNN = (0, 50)\n",
    "        coords_RNN = (250, 20)\n",
    "        coords_LSTM = (250, 50)\n",
    "        \n",
    "        #cv2.putText(overlay_image, 'RFC = '+prediction_RFC[0], coords_RFC, font, fontscale, color, thickness)\n",
    "        cv2.putText(overlay_image, 'RNN = '+prediction_RNN, coords_RNN, font, fontscale, color, thickness)\n",
    "        cv2.putText(overlay_image, 'ANN = '+prediction_ANN, coords_ANN, font, fontscale, color, thickness)\n",
    "        cv2.putText(overlay_image, 'CNN = '+prediction_CNN, coords_CNN, font, fontscale, color, thickness)\n",
    "        cv2.putText(overlay_image, 'LSTM = '+prediction_LSTM, coords_LSTM, font, fontscale, color, thickness)\n",
    "        \n",
    "        cv2.imshow('posenet', overlay_image)\n",
    "        \n",
    "        #result.write(overlay_image)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            #result.release()\n",
    "            break\n",
    "\n",
    "# print(cap.get(cv2.CAP_PROP_FPS)) -> frame rate\n",
    "print(f'time taken = {(time.time() - start)/60} mins')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('ANN')\n",
    "temp = [258.8809064423327, 1096.1560963877098, 247.6743717696233,\n",
    "       1103.16589102719, 237.49869866443905, 1111.8137990339578,\n",
    "       211.85583551118512, 1091.1299527575502, 197.20654406924518,\n",
    "       1082.078272799418, 240.85130642588032, 1042.566650771797,\n",
    "       224.6896357542929, 1012.5111997844092, 290.9416211163948,\n",
    "       1054.6050492941076, 324.71345306269507, 967.8415862775054,\n",
    "       305.6517183036645, 1068.0997568885393, 379.74106977782867,\n",
    "       972.2731288460247, 355.97467642055307, 1028.445288295582,\n",
    "       314.87228964965647, 906.4956276720152, 445.0320768951543,\n",
    "       1030.3417711458944, 406.206993282916, 952.7757588370903,\n",
    "       535.5605948946843, 1017.1907375791313, 486.6165264304237,\n",
    "       951.2660654441721]\n",
    "\n",
    "temp = pd.DataFrame(temp).T\n",
    "x = model.predict(temp)\n",
    "\n",
    "print(ohe_categories[np.argmax(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1596132693797"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}